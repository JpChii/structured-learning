{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of NLP\n",
    "\n",
    "In this notebook we're gonna recap basics of NLP. The topics include:\n",
    "\n",
    "1. Bag of words\n",
    "2. Stemming\n",
    "3. N-grams\n",
    "4. Tf-Idf\n",
    "5. Word2Vec\n",
    "\n",
    "We'll use Amazon Fine Food Reviews Dataset, and problem of sentiment analysis to understand these concepts.\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews\n",
    "\n",
    "Data includes(from kaggle):\n",
    "\n",
    "Reviews from Oct 1999 - Oct 2012\n",
    "* 568,454 reviews\n",
    "* 256,059 users\n",
    "* 74,258 products\n",
    "* 260 users with > 50 reviews\n",
    "\n",
    "Our target variable is Score in the dataset(review ratings):\n",
    "1. When score > 3 Positive\n",
    "2. When score < 3 Negative\n",
    "3. When Score = 3 Remove this from dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the sqlite file from dataset to load it into pandas\n",
    "# We'll remove score == 3 here\n",
    "conn = sqlite3.connect(\"data/database.sqlite\")\n",
    "\n",
    "# Read in the data\n",
    "filtered_data = pd.read_sql_query(\n",
    "    sql=\"\"\"\n",
    "Select * from Reviews where Score != 3\n",
    "\"\"\",\n",
    "con=conn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 525814 entries, 0 to 525813\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      525814 non-null  int64 \n",
      " 1   ProductId               525814 non-null  object\n",
      " 2   UserId                  525814 non-null  object\n",
      " 3   ProfileName             525814 non-null  object\n",
      " 4   HelpfulnessNumerator    525814 non-null  int64 \n",
      " 5   HelpfulnessDenominator  525814 non-null  int64 \n",
      " 6   Score                   525814 non-null  int64 \n",
      " 7   Time                    525814 non-null  int64 \n",
      " 8   Summary                 525814 non-null  object\n",
      " 9   Text                    525814 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 40.1+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.Score.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Score from ratings to postive and negative\n",
    "def partition(x):\n",
    "    if x > 3:\n",
    "        return \"positive\"\n",
    "    return \"negative\"\n",
    "\n",
    "actualScore = filtered_data[\"Score\"]\n",
    "postiveNegative = actualScore.map(partition)\n",
    "\n",
    "# Set the score variale in df to this\n",
    "filtered_data[\"Score\"] = postiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[\"Score\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Depluication\n",
    "\n",
    "Grabage in --> Garbage out\n",
    "\n",
    "There are duplicates in review for a single product's different flavours. Like a wafer cookies with differnet flavours. This is not useful as the data is duplicate and overfit the model or while splitting the data. Let's see a sample of this duplicate and remove them. This process is called deduplication.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_duplicates = pd.read_sql_query(\n",
    "    \"select * from reviews where UserId = 'AR5J8UI46CURR'\",\n",
    "    conn,\n",
    ")\n",
    "sample_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the context exaplined above code cell in these three datapoints, tiestamp, score etc are all the same. If you checkout the ProductId(ASIN) you'll notice they're different flavours of the same product. We'll remove the duplicates by keeping only a single review of a product by a single user.\n",
    "\n",
    "We'll sort the data and use drop_duplicates to perform this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOrting the data with productId\n",
    "sorted_data = filtered_data.sort_values(\n",
    "    by=\"ProductId\",\n",
    "    axis=0, # by Columns\n",
    "    ascending=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dedpuplication\n",
    "final = sorted_data.drop_duplicates(\n",
    "    subset={\n",
    "        \"UserId\",\n",
    "        \"ProfileName\",\n",
    "        \"Time\",\n",
    "        \"Text\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initally there were 500k+ data points now it's reduced to 360k+."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denominator should always be greater than numerator, let's check if there's any samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59301</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41159</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id   ProductId          UserId              ProfileName  \\\n",
       "59301  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "41159  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "       HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "59301                     3                       1  positive  1224892800   \n",
       "41159                     3                       2  positive  1212883200   \n",
       "\n",
       "                                            Summary  \\\n",
       "59301             Bought This for My Son at College   \n",
       "41159  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                    Text  \n",
       "59301  My son loves spaghetti so I didn't hesitate or...  \n",
       "41159  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[final[\"HelpfulnessNumerator\"] > final[\"HelpfulnessDenominator\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's remove these\n",
    "final = final[final[\"HelpfulnessNumerator\"] <= final[\"HelpfulnessDenominator\"]]\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "positive    307061\n",
       "negative     57110\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the class difference\n",
    "final.Score.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive classes are 6X of Negative classes. This is an imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Text to Vector?\n",
    "\n",
    "The two main feature for our problem is Summary and Text. But these feature are in Natural Language and ML models don't understand anything other than numbers. If we can convert these text to vectors, we can leverage linear algebra techniques we've learnt to classify the data points.\n",
    "\n",
    "We can cluster the datapoints and seperate them in d dimesnsional space of vectors.\n",
    "\n",
    "Thinking interms of linear algebra, if the two classes are seperable by a plane. We can use the equation of hyperplane with $W_T$ vector. If the point and $W_T$ vector are on the same side it's positive and negative vice versa.\n",
    "\n",
    "Intutions Behind this:\n",
    "1. Semantically similar text must be closer geometrically\n",
    "2. Semantic similarity is inversley proportion to distance.\n",
    "\n",
    "But how do we convert text to vectors?\n",
    "1. BOW\n",
    "2. TF-IDF\n",
    "3. Word2Vec\n",
    "4. Avg-Word2Vec\n",
    "5. TfIDF-Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "To create a Bag of Words(BOW):\n",
    "\n",
    "1. Create a set of all unique words in the corpus(All Documents or reviews in our ue case)\n",
    "2. Each word in the vocab forms a dimension\n",
    "3. We'll create a d-dimension vector(where d represents each unique word)\n",
    "\n",
    "**Characterisitcs of Bag of Words Vector:**\n",
    "\n",
    "To create a vector for a document - Fill the d-dimension vector with 1 for words present in document and 0 for words not in document. This is a binary BoW vector. These binary BoW vectors are sparse(has lot's of zeros). Why?\n",
    "\n",
    "Assume the 500k+ plus review, imagine the number of unique words across these reviews in it's entirety vs number of words in a single review. We'll end up with lots of zeros in the vector. Hence Sparse.\n",
    "\n",
    "Imageine D-Dimension vectors as row vector for filling it up.\n",
    "\n",
    "There's another type of BoW called count BoW where will fill the vector with number of times a word has occured instead of filling with 0's and 1's.\n",
    "\n",
    "**Problems with this approach:**\n",
    "\n",
    "* While we calculate distance between tow BoW's is just norm of ||v1-v2||, this is equivalent to the words not matching in two sentences. If only not is the differnce between two sentence, the distance will be small but it should be large due to the negation(not). The objective of achieving semantic similarity is lost here.\n",
    "* Semantic similarity between words is also lost(tasty and delecious have the same meaning, this is not captured here)\n",
    "* Sequence information between words in a sentece is also lost, based on the context a word might have different meaning.\n",
    "\n",
    "We'll have huge dimension vector, to reduce this we can remove the stopwords, stem the words.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The below steps can imporve BoW vectors.\n",
    "\n",
    "* Removal of stopwords - small dimension and meaningful vectors.\n",
    "* Stemming - tasty, tasteful, taste will become three different words in BoW vector. These can be converted to their common form (taste). This is called stemming. PorterStemmer and SnowballStemmer are some of them. ignores gramatical meaning of words. (Beatiful, Beauty is stemmed to Beaut)\n",
    "* Lemmatization - 1. Break similar sentences to it's root word, Break Sentences into words((group New York into a single word)).\n",
    "* lower case(To make Pasta, pasta the same in vector dimension)\n",
    "\n",
    "### Bi-grams, n-grams\n",
    "\n",
    "Creation of d-dimension vector with unique words is unigram(single word) BoW.\n",
    "\n",
    "Let's take two sample sentences:\n",
    "\n",
    "1. The device is good and affordabale\n",
    "2. The device is not good and affordable\n",
    "\n",
    "If we remove the stop words and create an uni-gram BoW what'll we get:\n",
    "\n",
    "1. device good affordable\n",
    "2. device good affordable\n",
    "\n",
    "yep even not is a stop word in nltk library, now if we create bow vectors for both these sentence and calculate similariy. We'll get they are same but that's not the case right? How can we overcome this...\n",
    "\n",
    "Instead of using single word we can use two words for creating a bi-gram BoW. Bi-Grams for 1st sentence will be `The device`, `device is`, `is good` and so on. If we create vectors now we'll have not good captured in second vector. This is not present in first vector increasing the distance.\n",
    "\n",
    "With Bi-grams, n-grams, we can capture part of sequence information lost in uni-gram vectors. But the down side as n-grams(n=1,2,3) the dimensionality will increase because there are tons of word conmbinations and sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "We'll now cover the weight calculation for each word in TF-IDF vector. The vector here is also same as BoW, d-dimension of uniqe words in the corpus. This is also a derivation of BoW with the how the vector is filled up changed with weights, instead of occurunces of words.\n",
    "\n",
    "To calculate weight for a single word in the vector, we need:\n",
    "1. Term Frequency(TF)\n",
    "2. Inverse Term Frequency(IDF)\n",
    "\n",
    "### Term Frequency\n",
    "\n",
    "TF(word, document) = Number of times Word occurs in document/Total number of words in the document\n",
    "\n",
    "* word occurence in a document can never be greater than total number of words, hence the result of division always lies between 0 and 1.\n",
    "* This can also be considered a probabality and can be defined as 0 <= TF(word, document) <= 1.\n",
    "* 1 will occur when only a single word is present in a document or all the words in the document are same.\n",
    "\n",
    "### inverse Document Frequency\n",
    "\n",
    "IDF(word, corpus) = log(Total Number of docs/Number of documents where the word occurs)\n",
    "$\\text{IDF}(t) = \\log \\left( \\frac{N}{n_i} \\right)$\n",
    "\n",
    "\n",
    "* Here we use the entire corpus instead of single document compared to IDF\n",
    "* Number of documents where the word occurs can never be greater than Total Number of docs\n",
    "* If there are 100 docs, if the word is present in all 100 docs, ex the might be present in all docs.\n",
    "IDF(the, corpus) = log(100/100) = log(1) = 0\n",
    "* Log never has values below zero(>=0) and the $\\left( \\frac{N}{n_i} \\right)$ is between 1 to N.\n",
    "* As ${n_i}$ increases IDF decreases, what does this mean. IDF will be really low for very common words like stopwords and very high for rare words. ${IDF} \\propto \\frac{1}{n_i}$\n",
    "* IDF >= 0.\n",
    "\n",
    "### Weights\n",
    "\n",
    "* The weight for a word is product of TF and IDF, that's hwy the name TF-IDF\n",
    "* More importance to rarer words in corpus(IDF)\n",
    "* More importance if a word is frequent in a Document(TF)\n",
    "\n",
    "*But still we haven't achieved our semantic similarity objective with this method as well*.\n",
    "\n",
    "### Why use log for IDF?\n",
    "\n",
    "* Stasically, frequency of words in y axis vs words in x axis will be a power law or paretto distribution, we can convert this to gaussian distribution using box-cox transforms(logarithm)\n",
    "* Intutivley, Assume the below two cases:\n",
    "    * IDF for The which will be in all documents wil be 1\n",
    "    * IDF for rare words like civilization, let's say for a 1000 document corpus, it's in 5 documents. 1000 / 5 = 250. Let's say TF is 0.7 = 175. log(250) * 0.7 -> 1.65.\n",
    "    * We can see points here. Without log the domination of IDF over TF and scale difference with and without log. For distance calculation algorithms(our objective of semantic similarity is a distance calculation algorithm) scale will have a big impact. As it's simply put dot product or cosine similarity. Even assuming during backpropogation with these large numbers we might end up with exploding gradients. These aRe some intuitve reasons to use log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec models are trained through deep learning supervision task from raw text or matrix factorizaiton. It's benefits are:\n",
    "\n",
    "* Dense embeddings(less zero's)\n",
    "* Higher dimension(more quality data)\n",
    "* Relationship are identified(man-->woman, king-->queen distance vectors will be neraly identical)\n",
    "\n",
    "This converts only word to vec. What we need is to convert reviews(sentences) to vector.\n",
    "\n",
    "1. Average W2V: Sum of all word vectors / Number of words\n",
    "2. TfIdf W2V: Summation of t_word_i * word2vec(word_i) / Summation of t_word_i - We calculate tf-idf of all words in the sentence(this is t). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's implement the code for all concepts above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() # from scikit-learn\n",
    "final_counts = count_vect.fit_transform(final[\"Text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 115281)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 115281)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 53 stored elements and shape (1, 115281)>\n",
      "  Coords\tValues\n",
      "  (0, 103749)\t3\n",
      "  (0, 113004)\t1\n",
      "  (0, 64507)\t1\n",
      "  (0, 22082)\t3\n",
      "  (0, 66253)\t1\n",
      "  (0, 71724)\t2\n",
      "  (0, 96473)\t2\n",
      "  (0, 63059)\t1\n",
      "  (0, 10401)\t1\n",
      "  (0, 65167)\t1\n",
      "  (0, 86314)\t2\n",
      "  (0, 59284)\t2\n",
      "  (0, 57052)\t2\n",
      "  (0, 103373)\t4\n",
      "  (0, 25403)\t1\n",
      "  (0, 9973)\t1\n",
      "  (0, 111527)\t1\n",
      "  (0, 85813)\t1\n",
      "  (0, 39477)\t1\n",
      "  (0, 7529)\t1\n",
      "  (0, 8302)\t2\n",
      "  (0, 53557)\t3\n",
      "  (0, 7734)\t1\n",
      "  (0, 24971)\t1\n",
      "  (0, 94619)\t1\n",
      "  :\t:\n",
      "  (0, 111991)\t1\n",
      "  (0, 57417)\t1\n",
      "  (0, 39520)\t1\n",
      "  (0, 89722)\t1\n",
      "  (0, 65217)\t1\n",
      "  (0, 7296)\t2\n",
      "  (0, 72824)\t1\n",
      "  (0, 113360)\t1\n",
      "  (0, 58762)\t1\n",
      "  (0, 94431)\t1\n",
      "  (0, 74846)\t1\n",
      "  (0, 59142)\t2\n",
      "  (0, 28971)\t1\n",
      "  (0, 7750)\t1\n",
      "  (0, 112660)\t1\n",
      "  (0, 104542)\t2\n",
      "  (0, 20386)\t1\n",
      "  (0, 112630)\t1\n",
      "  (0, 98814)\t1\n",
      "  (0, 19419)\t1\n",
      "  (0, 5093)\t1\n",
      "  (0, 47909)\t1\n",
      "  (0, 68341)\t1\n",
      "  (0, 112121)\t1\n",
      "  (0, 29981)\t1\n"
     ]
    }
   ],
   "source": [
    "print(final_counts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Inferences from code*\n",
    "\n",
    "* We can see 364K reviews with dimension of 115281, 115281 is the number of unique words in 364k+ reviews.\n",
    "* Space complexity for storing a matrix is $O(m*n)$ where m=364171(y axis) and n=1152819(x axis). For a single review 90% or more than that will all be zeros.\n",
    "* This complexity is really large just for zeros which is a huge waste of memory. \n",
    "* To overcome this, CountVectorizer implementation does this:--> Stores a dict of index(row no, col no) and count of k non zero values. Type - `csr_matrix`\n",
    "* Now the complexiety reduces to $O(row no, col no, count)$. $O(m*n)$ -> $O(3*k)$ --> $O(k)$\n",
    "* This implementation efficiency is directly proportational to sparsity of the matrix\n",
    "* Sparsity is more number of zeros in the vector. To calculate sparsity -> number of non zero cells / m * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.262445898788837e-09\n"
     ]
    }
   ],
   "source": [
    "# Sparsity of a single element\n",
    "# There are 53 non zero values in final_counts[0]\n",
    "sparsity = 53 / (final_counts.shape[0] * final_counts.shape[1])\n",
    "print(sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcessing Steps\n",
    "\n",
    "We'll implement a list of steps:\n",
    "1. Check if num is alphanum\n",
    "2. Check word length > 2\n",
    "3. stem word\n",
    "4. Remove html tags\n",
    "5. Remove punctuations\n",
    "6. Check if word is stop word or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\,'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\,'\n",
      "/var/folders/s2/zc28s499001f26bz7nbfmfhr0000gn/T/ipykernel_907/429179674.py:20: SyntaxWarning: invalid escape sequence '\\,'\n",
      "  \"\"\"_summary_\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop = set(stopwords.words(\"english\")) # stopwords\n",
    "sno = nltk.stem.SnowballStemmer(\"english\") # Stemmer\n",
    "\n",
    "def clean_html(sentence):\n",
    "    \"\"\"_summary_\n",
    "    Clean html tags from a sentence\n",
    "    Args:\n",
    "        sentence (str): Passage of NLT\n",
    "    \"\"\"\n",
    "    pattern = re.compile(\"<.*?>\") # Capture html pattern\n",
    "    cleaned_text = re.sub(pattern, \"  \", sentence) # replace this with double space instead of space, because we'll split the sentence by space to convert it to words\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_punctuation(sentence):\n",
    "    \"\"\"_summary_\n",
    "    Clean ?,!,',\",#,.,comma,),(,\\,/\n",
    "    Args:\n",
    "        sentence (str): Passage of NLT\n",
    "    \"\"\"\n",
    "    pattern_1 = r'[?|!|\\'|\"|#|]'\n",
    "    pattern_2 = r'[.|,|)|(|\\|/]'\n",
    "    cleaned_text = re.sub(pattern_1, r\"\", sentence)\n",
    "    cleaned_text = re.sub(pattern_2, \" \", cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean the text\n",
    "final_sentences = []\n",
    "all_positive_words = []\n",
    "all_negative_words = []\n",
    "\n",
    "# Iterate through text\n",
    "for idx, category, text in final[[\"Score\", \"Text\"]].itertuples():\n",
    "    # Store words for a single review\n",
    "    final_sentence = []\n",
    "    # Clean html\n",
    "    html_cleaned = clean_html(sentence=text)\n",
    "    for word in html_cleaned.split():\n",
    "        for cleaned_word in clean_punctuation(word).split():\n",
    "            # Check alphanum, word > 2 and word not in stop word\n",
    "            if (cleaned_word.isalpha()) and (len(cleaned_word) > 2) and (cleaned_word.lower() not in stop):\n",
    "                # Stem word\n",
    "                stemmed_word = sno.stem(cleaned_word.lower()).encode(\"utf8\")\n",
    "                final_sentence.append(stemmed_word)\n",
    "                if category == \"positive\":\n",
    "                    all_positive_words.append(stemmed_word)\n",
    "                else:\n",
    "                    all_negative_words.append(stemmed_word)\n",
    "            else:\n",
    "                continue\n",
    "    final_sentences.append(b\" \".join(final_sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[\"cleanedText\"] = final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>138706</th>\n",
       "      <td>150524</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>ACITT7DI6IDDL</td>\n",
       "      <td>shari zychinski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>939340800</td>\n",
       "      <td>EVERY book is educational</td>\n",
       "      <td>this witty little book makes my son laugh at l...</td>\n",
       "      <td>b'witti littl book make son laugh loud recit c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138688</th>\n",
       "      <td>150506</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A2IW4PEEKO2R0U</td>\n",
       "      <td>Tracy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1194739200</td>\n",
       "      <td>Love the book, miss the hard cover version</td>\n",
       "      <td>I grew up reading these Sendak books, and watc...</td>\n",
       "      <td>b'grew read sendak book watch realli rosi movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138689</th>\n",
       "      <td>150507</td>\n",
       "      <td>0006641040</td>\n",
       "      <td>A1S4A3IQ2MU7V4</td>\n",
       "      <td>sally sue \"sally sue\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1191456000</td>\n",
       "      <td>chicken soup with rice months</td>\n",
       "      <td>This is a fun way for children to learn their ...</td>\n",
       "      <td>b'fun way children learn month year learn poem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId            ProfileName  \\\n",
       "138706  150524  0006641040   ACITT7DI6IDDL        shari zychinski   \n",
       "138688  150506  0006641040  A2IW4PEEKO2R0U                  Tracy   \n",
       "138689  150507  0006641040  A1S4A3IQ2MU7V4  sally sue \"sally sue\"   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "138706                     0                       0  positive   939340800   \n",
       "138688                     1                       1  positive  1194739200   \n",
       "138689                     1                       1  positive  1191456000   \n",
       "\n",
       "                                           Summary  \\\n",
       "138706                   EVERY book is educational   \n",
       "138688  Love the book, miss the hard cover version   \n",
       "138689               chicken soup with rice months   \n",
       "\n",
       "                                                     Text  \\\n",
       "138706  this witty little book makes my son laugh at l...   \n",
       "138688  I grew up reading these Sendak books, and watc...   \n",
       "138689  This is a fun way for children to learn their ...   \n",
       "\n",
       "                                              cleanedText  \n",
       "138706  b'witti littl book make son laugh loud recit c...  \n",
       "138688  b'grew read sendak book watch realli rosi movi...  \n",
       "138689  b'fun way children learn month year learn poem...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364171"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store final table into an SqlLite table for future\n",
    "conn = sqlite3.connect(\"final.sqlite\")\n",
    "cursor = conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql(\n",
    "    \"Reviews\",\n",
    "    conn,\n",
    "    schema=None,\n",
    "    if_exists=\"replace\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Gram, N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_words_count = {}\n",
    "for positive_word in all_positive_words:\n",
    "    word = positive_word.decode()\n",
    "    all_positive_words_count[word] = all_positive_words_count.get(word, 0) + 1\n",
    "all_negative_words_count = {}\n",
    "for negative_word in all_negative_words:\n",
    "    # Decode to convert strings frmo binary string to text\n",
    "    word = negative_word.decode()\n",
    "    all_negative_words_count[word] = all_negative_words_count.get(word, 0) + 1\n",
    "\n",
    "# Sorting the count values\n",
    "all_positive_words_count = sorted(all_positive_words_count.items(), key=lambda item: item[1], reverse=True)\n",
    "all_negative_words_count = sorted(all_negative_words_count.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('like', 139429),\n",
       "  ('tast', 129047),\n",
       "  ('good', 112766),\n",
       "  ('flavor', 109624),\n",
       "  ('love', 107357)],\n",
       " [('tast', 34585),\n",
       "  ('like', 32330),\n",
       "  ('product', 28218),\n",
       "  ('one', 20569),\n",
       "  ('flavor', 19575)])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_positive_words_count[:5], all_negative_words_count[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the words common in negative and positive words(like, flavor). Intuitivley this is bad as there's no clear distinction between positive and negative reviews. By having bigrams or N-grams we can improve this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-Gram or N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the n_gram parameter in CountVectorizer\n",
    "# n_min and n_max, if this is 1,4. The vectorizer will create unigram, bigram, trigram and tetragram. As n_max increases the dimension(vocab size) of vector will also increase.\n",
    "# Let's see this in action\n",
    "bigram_vectorizer = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    ")\n",
    "bigram_final = bigram_vectorizer.fit_transform(final[\"Text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram counts dimension is 25.24433341140344x of BagOfWords dimension\n"
     ]
    }
   ],
   "source": [
    "# Let's see the increase in feature\n",
    "print(f\"Bigram counts dimension is {bigram_final.shape[1]/final_counts.shape[1]}x of BagOfWords dimension\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! We've a 25x increase in dimension for just unigram to bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf\n",
    "\n",
    "To implement TfIdf, we can just use the TfIdfVectorizer form scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_counts = tfidf_vectorizer.fit_transform(final[\"Text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_final.shape[1] == tfidf_counts.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910192"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tfidf features\n",
    "features = tfidf_vectorizer.get_feature_names_out()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features has the same length as dimensions, Assume this as a row vector all possible unique unigram and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14 count', '14 country', '14 crackers', '14 credit', '14 crude',\n",
       "       '14 cup', '14 cupcakes', '14 cups', '14 currants', '14 dad'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a function to see top 25 highest features/dimensions in a review\n",
    "def tfidf_top(review_counts, features, top_k=25):\n",
    "    \"\"\"_summary_\n",
    "    Function to get top k TfIdf Score features(unigram, bigram in our case)\n",
    "    Args:\n",
    "        review_counts (np.array): review arrary\n",
    "        features (_type_): TfIdf Features\n",
    "        top_k (int, optional): _description_. Defaults to 25.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get top score indexesx, reverse sort them, get top_k values\n",
    "    top_indexes = np.argsort(review_counts)[::-1][:top_k]\n",
    "    top_feats = [(features[i], review_counts[i]) for i in top_indexes]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = [\"features\", \"scores\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       " \twith 122 stored elements and shape (1, 2910192)>,\n",
       " array([[0., 0., 0., ..., 0., 0., 0.]]),\n",
       " (1, 2910192),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " (2910192,))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll convert a single review csr_matrix to array for sorting to the function above\n",
    "tfidf_counts[0, :], tfidf_counts[0, :].toarray(), tfidf_counts[0, :].toarray().shape, tfidf_counts[0, :].toarray()[0], tfidf_counts[0, :].toarray()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tfidf_top(review_counts=tfidf_counts[0, :].toarray()[0], features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>recite</td>\n",
       "      <td>0.234097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book</td>\n",
       "      <td>0.193848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roses love</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>about whales</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>introduces and</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recite from</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>classic book</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>book introduces</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>whales india</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>son laugh</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>recite it</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>silliness of</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>at loud</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the silliness</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>refrain he</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>india drooping</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this witty</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>memory when</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>loud recite</td>\n",
       "      <td>0.129413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>new words</td>\n",
       "      <td>0.125411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the refrain</td>\n",
       "      <td>0.125411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>witty little</td>\n",
       "      <td>0.125411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to recite</td>\n",
       "      <td>0.125411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>drooping roses</td>\n",
       "      <td>0.125411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>book makes</td>\n",
       "      <td>0.120369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           features    scores\n",
       "0            recite  0.234097\n",
       "1              book  0.193848\n",
       "2        roses love  0.129413\n",
       "3      about whales  0.129413\n",
       "4    introduces and  0.129413\n",
       "5       recite from  0.129413\n",
       "6      classic book  0.129413\n",
       "7   book introduces  0.129413\n",
       "8      whales india  0.129413\n",
       "9         son laugh  0.129413\n",
       "10        recite it  0.129413\n",
       "11     silliness of  0.129413\n",
       "12          at loud  0.129413\n",
       "13    the silliness  0.129413\n",
       "14       refrain he  0.129413\n",
       "15   india drooping  0.129413\n",
       "16       this witty  0.129413\n",
       "17      memory when  0.129413\n",
       "18      loud recite  0.129413\n",
       "19        new words  0.125411\n",
       "20      the refrain  0.125411\n",
       "21     witty little  0.125411\n",
       "22        to recite  0.125411\n",
       "23   drooping roses  0.125411\n",
       "24       book makes  0.120369"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe can be used to analyze classification results heuristically. Like we can find really bad samples of classifcation and investigate their scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're gonna use google's new article word2vec model. This is a 3.3Gb file which will occupy 9Gb ram. This will not fit in my 8Gb machine. Let's download a picked version of this.\n",
    "# From here - https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "# We can load the model using genism\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\n",
    "    \"models/GoogleNews-vectors-negative300.bin\", # Model path or name\n",
    "    binary=True, # Since we're using .bin\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This models is lookup table for vectors for their respective words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delicious', 0.8730390071868896),\n",
       " ('scrumptious', 0.8007041215896606),\n",
       " ('yummy', 0.7856924533843994),\n",
       " ('flavorful', 0.7420164346694946),\n",
       " ('delectable', 0.7385422587394714),\n",
       " ('juicy_flavorful', 0.7114803791046143),\n",
       " ('appetizing', 0.7017217874526978),\n",
       " ('crunchy_salty', 0.7012301087379456),\n",
       " ('flavourful', 0.691221296787262),\n",
       " ('flavoursome', 0.6857702732086182)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do some similarity checks for a word\n",
    "model.most_similar(\"tasty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.873039"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare words\n",
    "model.similarity(\"tasty\", \"delicious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector: (300,)\n",
      "Sample vector:\n",
      " [-2.39257812e-01 -1.25000000e-01 -2.07031250e-01  2.16796875e-01\n",
      " -3.19824219e-02  5.12695312e-02  8.78906250e-02 -1.43554688e-01\n",
      " -1.38671875e-01 -3.41796875e-02 -5.56640625e-02  1.50390625e-01\n",
      "  1.85546875e-01  1.17187500e-01 -2.85156250e-01  1.67968750e-01\n",
      " -1.01562500e-01 -1.01562500e-01  7.08007812e-02  6.68945312e-02\n",
      "  2.28515625e-01 -8.78906250e-02  3.06640625e-01  1.34765625e-01\n",
      "  2.42919922e-02 -1.54296875e-01 -2.92968750e-01  2.75390625e-01\n",
      "  2.00195312e-01  1.42822266e-02 -2.69531250e-01 -2.63671875e-01\n",
      "  3.65234375e-01 -1.55273438e-01 -3.02734375e-01 -3.22265625e-02\n",
      "  2.77343750e-01 -9.22851562e-02 -1.07421875e-01  1.25976562e-01\n",
      "  7.71484375e-02 -2.25585938e-01 -1.67968750e-01  3.33984375e-01\n",
      " -1.62109375e-01 -4.86328125e-01 -1.55273438e-01  1.07910156e-01\n",
      "  7.32421875e-02  1.23535156e-01 -1.80664062e-01  2.85156250e-01\n",
      "  1.92382812e-01  5.02929688e-02 -4.37011719e-02  1.90429688e-01\n",
      "  1.70898438e-01 -9.57031250e-02 -1.53320312e-01 -6.98852539e-03\n",
      " -7.27539062e-02  8.25195312e-02 -1.64062500e-01  7.72094727e-03\n",
      " -8.20312500e-02 -4.29687500e-01  1.19628906e-01 -8.17871094e-03\n",
      " -1.53320312e-01  7.66601562e-02  3.69140625e-01 -1.74804688e-01\n",
      "  1.26953125e-01  1.79687500e-01 -1.61132812e-01 -1.41601562e-01\n",
      " -3.00781250e-01 -3.07617188e-02 -9.22851562e-02  4.63867188e-02\n",
      " -4.73022461e-03  1.69677734e-02 -4.24804688e-02 -7.51953125e-02\n",
      " -7.37304688e-02 -1.55273438e-01 -5.23437500e-01  4.92187500e-01\n",
      "  2.05078125e-02 -2.40234375e-01  5.66406250e-02 -7.27539062e-02\n",
      " -5.93261719e-02 -2.92968750e-01 -2.67578125e-01  8.25195312e-02\n",
      "  1.05590820e-02  4.80957031e-02  9.96093750e-02  9.76562500e-03\n",
      " -1.17187500e-01 -9.32617188e-02 -4.46777344e-02  2.28515625e-01\n",
      " -9.47265625e-02 -2.83203125e-01 -2.12402344e-02 -2.14843750e-01\n",
      " -3.94531250e-01 -8.49609375e-02 -2.99072266e-02  1.66992188e-01\n",
      " -1.08886719e-01 -3.71093750e-01  3.39355469e-02 -6.22558594e-02\n",
      "  4.78515625e-02 -8.30078125e-03 -2.91015625e-01  3.54003906e-03\n",
      "  2.59765625e-01  8.25195312e-02  1.90429688e-01 -1.54296875e-01\n",
      " -2.39257812e-01  2.59765625e-01 -2.98828125e-01  1.82617188e-01\n",
      "  7.37304688e-02 -4.95605469e-02 -2.09960938e-01  4.29687500e-02\n",
      "  2.30468750e-01  2.06054688e-01 -3.55468750e-01  3.86718750e-01\n",
      " -1.21093750e-01 -1.68457031e-02  2.96875000e-01  3.24218750e-01\n",
      "  7.95898438e-02 -1.87500000e-01  3.46679688e-02 -1.26953125e-01\n",
      " -1.11328125e-01 -2.71484375e-01  7.22656250e-02  8.93554688e-02\n",
      " -1.36718750e-01 -3.78417969e-02 -1.54296875e-01 -6.59179688e-02\n",
      "  1.07421875e-01 -1.66015625e-01  5.71289062e-02 -4.95605469e-02\n",
      "  1.48437500e-01 -9.47265625e-02 -2.55859375e-01 -3.54003906e-02\n",
      " -2.63671875e-01 -1.12792969e-01  3.20312500e-01  9.09423828e-03\n",
      "  1.06933594e-01 -2.85156250e-01 -6.64062500e-02 -1.41601562e-01\n",
      " -7.86132812e-02 -1.24511719e-01 -1.29882812e-01 -9.03320312e-02\n",
      "  1.84570312e-01 -2.20703125e-01 -4.41406250e-01  3.18359375e-01\n",
      " -1.69921875e-01  1.84570312e-01  1.96289062e-01 -2.20947266e-02\n",
      "  2.89062500e-01 -2.71484375e-01 -4.27246094e-02 -1.28906250e-01\n",
      " -1.38671875e-01  8.78906250e-02 -4.12597656e-02  1.18164062e-01\n",
      " -4.88281250e-02 -1.31835938e-01 -1.41601562e-01  7.37304688e-02\n",
      "  2.04101562e-01 -2.87109375e-01 -1.13281250e-01  3.20312500e-01\n",
      " -7.22656250e-02 -2.35351562e-01  5.93261719e-02 -6.39648438e-02\n",
      " -1.01562500e-01 -4.66308594e-02 -7.12890625e-02  2.14843750e-01\n",
      " -4.24804688e-02 -2.63671875e-01 -7.12890625e-02  9.91210938e-02\n",
      " -1.52587891e-02  1.89453125e-01 -2.21679688e-01  9.91210938e-02\n",
      " -4.05883789e-03  3.86718750e-01  1.53320312e-01 -1.41601562e-01\n",
      "  1.55273438e-01  7.61718750e-02  1.33789062e-01 -1.70898438e-01\n",
      "  5.63964844e-02  2.57812500e-01 -3.00781250e-01  4.51660156e-02\n",
      " -3.11279297e-02  1.07910156e-01 -3.20312500e-01 -6.25000000e-02\n",
      "  1.08032227e-02  1.14746094e-01 -1.25976562e-01  1.75781250e-01\n",
      "  4.71191406e-02  3.26171875e-01  3.90625000e-01 -4.06250000e-01\n",
      "  2.69531250e-01  1.90429688e-01 -3.96484375e-01  9.47265625e-02\n",
      "  4.62890625e-01 -1.88476562e-01 -6.03027344e-02 -8.93554688e-02\n",
      "  2.45117188e-01 -6.88476562e-02 -3.56445312e-02  2.25585938e-01\n",
      "  1.23901367e-02  1.34765625e-01 -2.65625000e-01  8.78906250e-02\n",
      "  1.88476562e-01  1.56250000e-01  4.34570312e-02 -7.76367188e-02\n",
      " -2.03125000e-01 -8.23974609e-03 -2.73437500e-02 -1.22558594e-01\n",
      "  7.22656250e-02  8.23974609e-03 -2.92968750e-01  7.47070312e-02\n",
      "  2.53677368e-04  5.68847656e-02 -3.06640625e-01 -8.98437500e-02\n",
      " -2.38281250e-01  3.46679688e-02  5.46875000e-01  2.31445312e-01\n",
      " -2.81250000e-01  1.00097656e-01  3.86718750e-01  1.46484375e-01\n",
      " -1.29882812e-01 -3.16406250e-01 -1.77001953e-02 -1.74804688e-01\n",
      " -1.16210938e-01  1.44531250e-01 -2.48046875e-01  1.45507812e-01\n",
      " -2.22656250e-01  1.18652344e-01 -1.80664062e-01  1.00585938e-01\n",
      " -1.52343750e-01  3.90625000e-01  1.45507812e-01 -5.10253906e-02\n",
      " -1.76757812e-01  9.81445312e-02  2.50000000e-01 -1.41601562e-01\n",
      " -1.26342773e-02 -9.57031250e-02  6.29882812e-02 -1.55273438e-01]\n"
     ]
    }
   ],
   "source": [
    "# Let's try to find the vector for a word\n",
    "sample_vector = model.get_vector(\"Tasty\")\n",
    "print(f\"Shape of vector: {sample_vector.shape}\")\n",
    "print(f\"Sample vector:\\n {sample_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'tasti' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Few stemmed words might not be present int the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mget_vector(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtasti\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'tasti' not present\""
     ]
    }
   ],
   "source": [
    "# Few stemmed words might not be present int the model\n",
    "model.get_vector(\"tasti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To overcome this we can train our own word2vec model from our corpus\n",
    "# To do this we need to pass data in this format [[word1,word2], [word1,word2]] -> [[sentence1], [sentence2]] -> Where each sentence is split into words\n",
    "# We won't remove stopwords because, we'll lose not.\n",
    "word2_vec_reviews = []\n",
    "for review in final[\"Text\"].values:\n",
    "    filtered_sentence = []\n",
    "    # Clean html\n",
    "    html_cleaned_review = clean_html(review)\n",
    "    # Split word\n",
    "    for word in html_cleaned_review.split():\n",
    "        # Split based on punc\n",
    "        for cleaned_word in clean_punctuation(word).split():\n",
    "            if (cleaned_word.isalpha()):\n",
    "                filtered_sentence.append(cleaned_word)\n",
    "            else:\n",
    "                continue\n",
    "    word2_vec_reviews.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'witty', 'little', 'book', 'makes', 'my', 'son', 'laugh', 'at', 'loud', 'i', 'recite', 'it', 'in', 'the', 'car', 'as', 'were', 'driving', 'along', 'and', 'he', 'always', 'can', 'sing', 'the', 'refrain', 'hes', 'learned', 'about', 'whales', 'India', 'drooping', 'i', 'love', 'all', 'the', 'new', 'words', 'this', 'book', 'introduces', 'and', 'the', 'silliness', 'of', 'it', 'all', 'this', 'is', 'a', 'classic', 'book', 'i', 'am', 'willing', 'to', 'bet', 'my', 'son', 'will', 'STILL', 'be', 'able', 'to', 'recite', 'from', 'memory', 'when', 'he', 'is', 'in', 'college']\n",
      "this witty little book makes my son laugh at loud i recite it in the car as were driving along and he always can sing the refrain hes learned about whales India drooping i love all the new words this book introduces and the silliness of it all this is a classic book i am willing to bet my son will STILL be able to recite from memory when he is in college\n"
     ]
    }
   ],
   "source": [
    "print(word2_vec_reviews[0])\n",
    "print(\" \".join(word2_vec_reviews[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "    word2_vec_reviews, # List of list sentences\n",
    "    min_count=5, # word has to occur 5 times to create a vector\n",
    "    vector_size=50, # Dimension of the vector\n",
    "    workers=2, # Number of workers to build this model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "w2v_model.build_vocab(word2_vec_reviews, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(636393154, 836628210)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "w2v_model.train(word2_vec_reviews, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s2/zc28s499001f26bz7nbfmfhr0000gn/T/ipykernel_907/2419709253.py:2: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "# Make the mode more memeory efficient\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.16849469,  0.16382068, -0.00642152, -0.35406202, -0.0265453 ,\n",
       "         0.32891715, -0.10842107, -0.00634135,  0.03764758, -0.19081114,\n",
       "        -0.02937614, -0.08751374,  0.16647403,  0.079576  ,  0.2227623 ,\n",
       "        -0.01321285, -0.26592404,  0.23118702,  0.0995752 ,  0.01758653,\n",
       "        -0.09244239, -0.02822721,  0.15739277, -0.12555908, -0.08097733,\n",
       "         0.14662708, -0.27878094,  0.14695078, -0.11488359,  0.00485369,\n",
       "        -0.05462616, -0.20081028, -0.1395006 , -0.02717623,  0.10202011,\n",
       "        -0.08236805, -0.009123  , -0.03384229,  0.1199767 , -0.12020289,\n",
       "         0.21988836, -0.07583645,  0.08863829,  0.2128078 , -0.00474559,\n",
       "        -0.00857432, -0.0487239 , -0.06127046, -0.1365328 ,  0.02739566],\n",
       "       dtype=float32),\n",
       " (50,))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.get_vector(\"taste\"), w2v_model.wv.get_vector(\"taste\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've created a word2vec model for our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word to vec for a sentence -> Average of all word vectors in a sentence\n",
    "avg_sent_vectors = []\n",
    "# Using cleaned text\n",
    "for sentence in final[\"Text\"].values:\n",
    "    # Initial sent_vector with zeros\n",
    "    sent_vector = np.zeros(50)\n",
    "    cnt_vectors = 0\n",
    "    for word in sentence.split():\n",
    "        # w2v_model.wv.index_to_key -> vocab\n",
    "        if word in w2v_model.wv.index_to_key:\n",
    "            word_vec = w2v_model.wv.get_vector(word)\n",
    "            sent_vector += word_vec\n",
    "            cnt_vectors += 1\n",
    "    sent_vector /= cnt_vectors\n",
    "    avg_sent_vectors.append(sent_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf Word2Vec\n",
    "\n",
    "We can get IDF scores from tfidf model, calculate tf scores from the sentence for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting idf scores for words\n",
    "idf_dict = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "print(idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_sent_vectors = []\n",
    "for sentence in final[\"Text\"].values:\n",
    "    # Initial sent_vector with zeros\n",
    "    sent_vector = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    tf_sum = 0\n",
    "    for word in sentence:\n",
    "        if word in w2v_model.wv.index_to_key and word in idf_dict:\n",
    "            # Calculate word2vec\n",
    "            word_vec = w2v_model.wv.get_vector(word)\n",
    "            # Calculate tfidf score\n",
    "            tf_idf = idf_dict.get(word) * sentence.count(word) / len(sentence)\n",
    "            # Summation of tfidf scores for average\n",
    "            weight_sum += tf_idf\n",
    "            # Summation of sent vector\n",
    "            sent_vector += (tf_idf * word_vec)\n",
    "    if weight_sum != 0:\n",
    "        sent_vector /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
